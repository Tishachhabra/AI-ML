{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protodash Explanations for Text data\n",
    "\n",
    "In the example shown in this notebook, we train a text classifier based on [UCI SMS dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) to distinguish 'SPAM' and 'HAM' (i.e. non spam) SMS messages. We then use the ProtodashExplainer to obtain spam and ham prototypes based on the labels assigned by the text classifier. \n",
    "\n",
    "In order to run this notebook, please: \n",
    "1. Download [UCI SMS dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) dataset and place the directory 'smsspamcollection' in the location of this notebook. \n",
    "2. Place glove embedding file \"glove.6B.100d.txt\" in the location of this notebook. This can be downloaded from [here](https://nlp.stanford.edu/projects/glove/) \n",
    "3. Create 2 folders: \"results\" and \"logs\" in the location of this notebook (these are used to store training logs). \n",
    "4. The models trained in this notebook can also be accessed from [here](https://github.com/IBM/AIX360/tree/master/aix360/models/protodash) if required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Train a LSTM classifier on SMS dataset\n",
    "We train a LSTM model to label the dataset as spam / ham. The model is based on the following code: https://www.thepythoncode.com/article/build-spam-classifier-keras-python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import keras_metrics # for recall and precision metrics\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n",
    "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
    "TEST_SIZE = 0.25 # ratio of testing set\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20 # number of epochs\n",
    "\n",
    "# to convert labels to integers and vice-versa\n",
    "label2int = {\"ham\": 0, \"spam\": 1}\n",
    "int2label = {0: \"ham\", 1: \"spam\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "combined_df = pd.read_csv(\"C:/Users/hp/Documents/ML/jupyter notebook/Datasets/SMSSpamCollection.txt\", delimiter='\\t',header=None)\n",
    "combined_df.columns = ['label', 'text']\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide in text and labels list from original df\n",
    "X = combined_df['text'].values.tolist()\n",
    "y = combined_df['label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenization, extract features from whole X list  \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)                         \n",
    "\n",
    "# convert to sequence of integers\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences at the beginning of each sequence with 0's\n",
    "# for example if SEQUENCE_LENGTH=4:\n",
    "# [[5, 3, 2], [5, 1, 2, 3], [3, 4]]\n",
    "# will be transformed to:\n",
    "# [[0, 5, 3, 2], [5, 1, 2, 3], [0, 0, 3, 4]]\n",
    "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "# convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [ label2int[label] for label in y ]\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and shuffle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(tokenizer, dim=100):\n",
    "    embedding_index = {}\n",
    "    with open(f\"glove.6B.{dim}d.txt\", encoding='utf8') as f:\n",
    "        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_index[word] = vectors\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found will be 0s\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(tokenizer, lstm_units):\n",
    "    \"\"\"\n",
    "    Constructs the model,\n",
    "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
    "    \"\"\"\n",
    "    # get the GloVe embedding vectors\n",
    "    embedding_matrix = get_embedding_vectors(tokenizer)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index)+1,\n",
    "              EMBEDDING_SIZE,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              input_length=SEQUENCE_LENGTH))\n",
    "\n",
    "    model.add(LSTM(lstm_units, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    # compile as rmsprop optimizer\n",
    "    # aswell as with recall metric\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GloVe: 400000it [00:22, 17569.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          901200    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1018706 (3.89 MB)\n",
      "Trainable params: 117506 (459.01 KB)\n",
      "Non-trainable params: 901200 (3.44 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# constructs the model with 128 LSTM units\n",
    "model = get_model(tokenizer=tokenizer, lstm_units=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model or load trained model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 100)          901000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1018506 (3.89 MB)\n",
      "Trainable params: 117506 (459.01 KB)\n",
      "Non-trainable params: 901000 (3.44 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "to_train = False\n",
    "\n",
    "if (to_train): \n",
    "\n",
    "    # initialize our ModelCheckpoint and TensorBoard callbacks\n",
    "    # model checkpoint for saving best weights\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\"results/spam_classifier_{val_loss:.2f}\", save_best_only=True,verbose=1)\n",
    "    # for better visualization\n",
    "    tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n",
    "    # print our data shapes\n",
    "    print(\"X_train.shape:\", X_train.shape)\n",
    "    print(\"X_test.shape:\", X_test.shape)\n",
    "    print(\"y_train.shape:\", y_train.shape)\n",
    "    print(\"y_test.shape:\", y_test.shape)\n",
    "    # train the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),batch_size=BATCH_SIZE, epochs=EPOCHS,callbacks=[tensorboard, model_checkpoint],verbose=1)\n",
    "      # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"sms-lstm-forprotodash.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"sms-lstm-forprotodash.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "        \n",
    "else: \n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open(\"sms-lstm-forprotodash.json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # load weights into new model\n",
    "    model.load_weights(\"sms-lstm-forprotodash.h5\")\n",
    "    print(\"Loaded model from disk\")  \n",
    "    # print model \n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 4s 61ms/step - loss: 0.0766 - accuracy: 0.9813\n",
      "[+] Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "# get the loss and metrics\n",
    "result = model.evaluate(X_test, y_test)\n",
    "\n",
    "# extract those\n",
    "loss = result[0]\n",
    "accuracy = result[1]\n",
    "\n",
    "print(f\"[+] Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Get model predictions for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(doclist):\n",
    "    \n",
    "    sequence = tokenizer.texts_to_sequences(doclist)\n",
    "    \n",
    "    # pad the sequence\n",
    "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
    "    # get the prediction as one-hot encoded vector\n",
    "    prediction = model.predict(sequence)\n",
    "    \n",
    "    return (prediction)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 976ms/step\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Congratulations! you have won 100,000$ this week, click here to claim fast\"\n",
    "pred = get_predictions([text1])\n",
    "print(int2label [ np.argmax(pred, axis=1)[0] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step\n",
      "ham\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Hi man, I was wondering if we can meet tomorrow.\"\n",
    "pred = get_predictions([text2,text1])\n",
    "print(int2label [ np.argmax(pred, axis=1)[0] ] )\n",
    "print(int2label [ np.argmax(pred, axis=1)[1] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174/174 [==============================] - 10s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "doclist = combined_df['text'].values.tolist()\n",
    "one_hot_prediction = get_predictions(doclist[:-10])\n",
    "label_prediction = np.argmax(one_hot_prediction, axis=1)\n",
    "\n",
    "# 0: ham, 1:spam\n",
    "idx_ham = (label_prediction == 0)\n",
    "idx_spam = (label_prediction == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 3. Use protodash explainer to compute spam and ham prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from aix360.algorithms.protodash import ProtodashExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to vectors using TF-IDF for use in explainer\n",
    "\n",
    "We use TF-IDF vectors for scalability reasons as the original embedding vector for a full sentence can be quite large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5562, 8707)\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(doclist[:-10])\n",
    "\n",
    "vec = vectorizer.transform(doclist[:-10])\n",
    "docvec = vec.toarray()\n",
    "print(docvec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate spam and ham messages and corrsponding vectors\n",
    "new_df=combined_df[:-10]\n",
    "docvec_spam = docvec[idx_spam, :]\n",
    "docvec_ham = docvec[idx_ham, :]\n",
    "df_spam = new_df[idx_spam]['text']\n",
    "df_ham = new_df[idx_ham]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(717,)\n",
      "(4845,)\n"
     ]
    }
   ],
   "source": [
    "print(df_spam.shape)\n",
    "print(df_ham.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute prototypes for spam and ham datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ProtodashExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "\n",
    "# call protodash explainer\n",
    "# S contains indices of the selected prototypes\n",
    "# W contains importance weights associated with the selected prototypes \n",
    "(W_spam, S_spam, _) = explainer.explain(docvec_spam, docvec_spam, m=m)\n",
    "(W_ham, S_ham, _) = explainer.explain(docvec_ham, docvec_ham, m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prototypes from index\n",
    "df_spam_prototypes = df_spam.iloc[S_spam].copy()\n",
    "df_ham_prototypes = df_ham.iloc[S_ham].copy()\n",
    "\n",
    "#normalize weights\n",
    "W_spam = np.around(W_spam/np.sum(W_spam), 2) \n",
    "W_ham = np.around(W_ham/np.sum(W_ham), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAM prototypes with weights:\n",
      "----------------------------\n",
      "0.13 We tried to call you re your reply to our sms for a video mobile 750 mins UNLIMITED TEXT + free camcorder Reply of call 08000930705 Now\n",
      "0.13 You have WON a guaranteed £1000 cash or a £2000 prize.To claim yr prize call our customer service representative on\n",
      "0.12 Get ur 1st RINGTONE FREE NOW! Reply to this msg with TONE. Gr8 TOP 20 tones to your phone every week just £1.50 per wk 2 opt out send STOP 08452810071 16\n",
      "0.1 December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906\n",
      "0.09 Dear Voucher Holder, To claim this weeks offer, at you PC please go to http://www.e-tlp.co.uk/expressoffer Ts&Cs apply. To stop texts, txt STOP to 80062\n",
      "0.09 URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\n",
      "0.08 Free entry in 2 a weekly comp for a chance to win an ipod. Txt POD to 80182 to get entry (std txt rate) T&C's apply 08452810073 for details 18+\n",
      "0.09 Urgent! call 09066350750 from your landline. Your complimentary 4* Ibiza Holiday or 10,000 cash await collection SAE T&Cs PO BOX 434 SK3 8WP 150 ppm 18+ \n",
      "0.08 YES! The only place in town to meet exciting adult singles is now in the UK. Txt CHAT to 86688 now! 150p/Msg.\n",
      "0.08 What do U want for Xmas? How about 100 free text messages & a new video phone with half price line rental? Call free now on 0800 0721072 to find out more!\n"
     ]
    }
   ],
   "source": [
    "print(\"SPAM prototypes with weights:\")\n",
    "print(\"----------------------------\")\n",
    "for i in range(m):\n",
    "    print(W_spam[i], df_spam_prototypes.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAM prototypes with weights:\n",
      "----------------------------\n",
      "0.14 The last thing i ever wanted to do was hurt you. And i didn't think it would have. You'd laugh, be embarassed, delete the tag and keep going. But as far as i knew, it wasn't even up. The fact that you even felt like i would do it to hurt you shows you really don't know me at all. It was messy wednesday, but it wasn't bad. The problem i have with it is you HAVE the time to clean it, but you choose not to. You skype, you take pictures, you sleep, you want to go out. I don't mind a few things here and there, but when you don't make the bed, when you throw laundry on top of it, when i can't have a friend in the house because i'm embarassed that there's underwear and bras strewn on the bed, pillows on the floor, that's something else. You used to be good about at least making the bed.\n",
      "0.11 What do u want when i come back?.a beautiful necklace as a token of my heart for you.thats what i will give but ONLY to MY WIFE OF MY LIKING.BE THAT AND SEE..NO ONE can give you that.dont call me.i will wait till i come.\n",
      "0.12 It‘s £6 to get in, is that ok?\n",
      "0.12 Babe ! What are you doing ? Where are you ? Who are you talking to ? Do you think of me ? Are you being a good boy? Are you missing me? Do you love me ?\n",
      "0.1 So you think i should actually talk to him? Not call his boss in the morning? I went to this place last year and he told me where i could go and get my car fixed cheaper. He kept telling me today how much he hoped i would come back in, how he always regretted not getting my number, etc.\n",
      "0.08 Dude. What's up. How Teresa. Hope you have been okay. When i didnt hear from these people, i called them and they had received the package since dec  &lt;#&gt; . Just thot you'ld like to know. Do have a fantastic year and all the best with your reading. Plus if you can really really Bam first aid for Usmle, then your work is done.\n",
      "0.09 Dont hesitate. You know this is the second time she has had weakness like that. So keep i notebook of what she eat and did the day before or if anything changed the day before so that we can be sure its nothing\n",
      "0.08 Sorry, I'll call you  later. I am in meeting sir.\n",
      "0.08 Just got to  &lt;#&gt;\n",
      "0.08 Yes but can we meet in town cos will go to gep and then home. You could text at bus stop. And don't worry we'll have finished by march … ish!\n"
     ]
    }
   ],
   "source": [
    "print(\"HAM prototypes with weights:\")\n",
    "print(\"----------------------------\")\n",
    "for i in range(m):\n",
    "    print(W_ham[i], df_ham_prototypes.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a message, look for similar messages that are classified as spam by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "sample_text = df_spam.iloc[k]\n",
    "sample_vec = docvec_spam[k]\n",
    "sample_vec = sample_vec.reshape(1, sample_vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "(1, 8707)\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)\n",
    "print(sample_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_spam_other = docvec_spam[np.arange(docvec_spam.shape[0]) != k, :] \n",
    "df_spam_other = df_spam.iloc[np.arange(docvec_spam.shape[0]) != k].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample spam text and find samples similar to it. \n",
    "(W1_spam, S1_spam, _) = explainer.explain(sample_vec, docvec_spam_other, m=m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize weights\n",
    "W1_spam = np.around(W1_spam/np.sum(W1_spam), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([170, 255, 399, 445, 673, 512,  35,  58,  49,  37])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text\n",
      "-------------\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "Similar SPAM prototypes:\n",
      "------------------------\n",
      "1.0 Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "0.0 Free entry in 2 a weekly comp for a chance to win an ipod. Txt POD to 80182 to get entry (std txt rate) T&C's apply 08452810073 for details 18+\n",
      "-0.0 Free entry in 2 a weekly comp for a chance to win an ipod. Txt POD to 80182 to get entry (std txt rate) T&C's apply 08452810073 for details 18+\n",
      "-0.0 Wan2 win a Meet+Greet with Westlife 4 U or a m8? They are currently on what tour? 1)Unbreakable, 2)Untamed, 3)Unkempt. Text 1,2 or 3 to 83049. Cost 50p +std text\n",
      "-0.0 ree entry in 2 a weekly comp for a chance to win an ipod. Txt POD to 80182 to get entry (std txt rate) T&C's apply 08452810073 for details 18+\n",
      "-0.0 Wan2 win a Meet+Greet with Westlife 4 U or a m8? They are currently on what tour? 1)Unbreakable, 2)Untamed, 3)Unkempt. Text 1,2 or 3 to 83049. Cost 50p +std text\n",
      "-0.0 Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry 2 100 wkly draw txt MUSIC to 87066 TnCs www.Ldew.com1win150ppmx3age16\n",
      "-0.0 FREE entry into our £250 weekly competition just text the word WIN to 80086 NOW. 18 T&C www.txttowin.co.uk\n",
      "-0.0 Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry 2 100 wkly draw txt MUSIC to 87066 TnCs www.Ldew.com1win150ppmx3age16\n",
      "0.0 Hey I am really horny want to chat or see me naked text hot to 69698 text charged at 150pm to unsubscribe text stop 69698\n"
     ]
    }
   ],
   "source": [
    "# similar spam prototypes\n",
    "print(\"original text\")\n",
    "print(\"-------------\")\n",
    "print(sample_text)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Similar SPAM prototypes:\")\n",
    "print(\"------------------------\")\n",
    "m = 10\n",
    "for i in range(m):\n",
    "    print(W1_spam[i], df_spam_other.iloc[S1_spam[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "Note several spam messages repeat in the dataset as these may have been sent by the same entity to multiple users. As a consequence, the explainer retireves these. Try with a different k above to see prototypes corrsponding to other sample messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a ham message, look for similar messages that are classified as spam by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "sample_text = df_ham.iloc[k]\n",
    "sample_vec = docvec_ham[k]\n",
    "sample_vec = sample_vec.reshape(1, sample_vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nah I don't think he goes to usf, he lives around here though\n",
      "(1, 8707)\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)\n",
    "print(sample_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_ham_other = docvec_ham[np.arange(docvec_ham.shape[0]) != k, :] \n",
    "df_ham_other = df_ham.iloc[np.arange(docvec_ham.shape[0]) != k].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample spam text and find samples similar to it. \n",
    "(W1_ham, S1_ham, _) = explainer.explain(sample_vec, docvec_ham_other, m=m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize weights\n",
    "W1_ham = np.around(W1_ham/np.sum(W1_ham), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 925, 3271,  832,  345, 2352, 3826, 3586, 2137, 2465,  946])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text\n",
      "-------------\n",
      "Nah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "Similar HAM prototypes:\n",
      "------------------------\n",
      "0.13 I don't think he has spatula hands!\n",
      "0.13 Aight, let me know when you're gonna be around usf\n",
      "0.1 If he started searching he will get job in few days.he have great potential and talent.\n",
      "0.09 None of that's happening til you get here though\n",
      "0.11 Nah, I'm a perpetual DD\n",
      "0.1 Yes just finished watching days of our lives. I love it.\n",
      "0.1 Babe! How goes that day ? What are you up to ? I miss you already, my Love ... * loving kiss* ... I hope everything goes well.\n",
      "0.09 S.i think he is waste for rr..\n",
      "0.08 Were trying to find a Chinese food place around here\n",
      "0.08 Awesome, think we can get an 8th at usf some time tonight?\n"
     ]
    }
   ],
   "source": [
    "# similar spam prototypes\n",
    "print(\"original text\")\n",
    "print(\"-------------\")\n",
    "print(sample_text)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Similar HAM prototypes:\")\n",
    "print(\"------------------------\")\n",
    "m = 10\n",
    "for i in range(m):\n",
    "    print(W1_ham[i], df_ham_other.iloc[S1_ham[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
